{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 25000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_EVERY = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(env.observation_space.high)\n",
    "# print(env.observation_space.low)\n",
    "# print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRETE_OS_SIZE = [20]  * len(env.observation_space.high)\n",
    "discrete_os_win_size =(env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES // 2\n",
    "\n",
    "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(discrete_os_win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2,high=0,size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "# print(q_table.shape)\n",
    "# print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rewards = []\n",
    "aggr_ep_rewards = {'ep': [],'avg': [],'min': [], 'max': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    discrete_state = (state -env.observation_space.low) / discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(discrete_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.argmax(q_table[discrete_state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(episode)\n",
    "        render = True\n",
    "    else:\n",
    "        render = False\n",
    "         \n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done =False\n",
    "    while not done:\n",
    "        \n",
    "        \n",
    "        if np.random.random() > epsilon:\n",
    "            action =np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            action = np.random.randint(0,env.action_space.n)\n",
    "            \n",
    "        new_state,reward,done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if not done:\n",
    "            max_feature_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action, )]\n",
    "            new_q =(1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_feature_q)\n",
    "            q_table[discrete_state+(action,)] = new_q\n",
    "        elif new_state[0] >=env.goal_position:\n",
    "            #print(f\"we made it on episode {episode}\")\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "        \n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "    \n",
    "    ep_rewards.append(episode_reward)\n",
    "    \n",
    "#     if not episode %10:\n",
    "#         np.save(f\"qtables/{episode}-qtable.npy\", q_table)\n",
    "    \n",
    "    if not episode % SHOW_EVERY:\n",
    "        \n",
    "        avarage_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n",
    "        aggr_ep_rewards['ep'].append(episode)\n",
    "        aggr_ep_rewards['avg'].append(avarage_reward)\n",
    "        aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
    "        aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
    "        \n",
    "        #print(f\"Episode:{episode},avg: {average_reward}, min: {min(ep_rewards[-SHOW_EVERY:]},max: {max(ep_rewards[-SHOW_EVERY:])}\")\n",
    "    \n",
    "    #if episode % SHOW_EVERY == 0:\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aggr_ep_rewards['ep'],aggr_ep_rewards['avg'],label = \"avg\")\n",
    "plt.plot(aggr_ep_rewards['ep'],aggr_ep_rewards['min'],label = \"min\")\n",
    "plt.plot(aggr_ep_rewards['ep'],aggr_ep_rewards['max'],label = \"max\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
